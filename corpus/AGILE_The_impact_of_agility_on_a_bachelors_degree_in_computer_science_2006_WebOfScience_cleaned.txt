The Impact of Agility on a Bachelor’s Degree in Computer Science


Abstract
Shippensburg University offers a Computer Science degree with the choice of five concentrations including Software Engineering. We first introduced agility as an XP component in part of our two-semester product development sequence. Within the software engineering concentration, agile concepts spread into our course on Software Metrics and Process Management. With that experience, our faculty have become interested in including agile concepts into other courses. This paper describes how a number of our undergraduate courses, the content of our computer science core, and our department’s general health have been affected by agile techniques and philosophies.

1. Introduction
Shippensburg University has a long history of a quality undergraduate program (in fact, we hope to be accredited by ABET this year). One of the hallmarks of our program is its practical focus. While some of our students go on to graduate school, most will get a job when they finish their degree. We serve that population by providing as much hands on experience and as many real world examples as we can manage. In addition, many of our students work in computer science related jobs and internships. This gives them excellent perspective on software engineering topics and brings more real world perspectives into our classrooms.
With this practical focus, our program used to climax with the typical two-semester software development team projects. We tried to ensure that the projects had real customers and we followed a waterfall model: requirements analysis and design in the first semester and implementation and test in the second semester.
This paper will cover the introduction and impact of agile methodologies within our curriculum. Section 2 will discuss how our software engineering courses have evolved. Section 3 will discuss how agile methodologies have impacted student thinking in our metrics/management course. Section 4 will discuss the use of agile methods in our CS I and CS II courses. Section 5 will discuss the future of agile methodologies in our curriculum and section 6 will conclude the paper.


2. First Forays into XP
Our first experience with agile techniques began in 2000 when we changed the format of the two-semester project course. Instead of producing one release of the product using only a waterfall process, we made the first semester produce a first release using waterfall and the second semester produce a second release of the same project using XP. The goal of this change was to show our students extremes of the spectrum of development processes. We wanted them to learn these concepts:
- XP is no less disciplined than waterfall.
-The process must fit within the culture and capabilities of the organization.
- The process has non-technical effects on things like motivation and team cohesiveness.
- No process is optimal for every situation; good software engineers are always looking to improve the process by focusing on the weaknesses of the current process.
While this change was successful [5], due to informal discussions with the students we had some concerns with the sequential nature of these courses. In particular, students wondered whether XP would work for the initial release of the project. Some of the students suspected that the planning they did for the first release was critical to making XP work in the subsequent release. In addition, some of the students thought that the increased success they had with XP was a result of the fact that their team had functioned together in the previous semester. Therefore, the fact that they felt less stress and more accomplishment with XP might have been a result of their teamwork skills improving. Based on these informal discussions with the students it was determined that XP should stay, but as a completely separate course.
2.1. Making XP Official
With the success of our trial courses in XP, we made an official change to the software engineering curriculum. The two-semester sequence was divided into two separate courses: plan-driven development and Extreme Programming. These courses are no longer sequential (the students can take them in either order and the projects do not continue from one course to the next). This change addressed the concerns we had with the sequential courses and made agility specific in our course descriptions (the previous courses had been called Software Engineering I and II even after we had introduced XP). These courses have run successfully for three years and they are now taught by three of our faculty members.
A formal evaluation was done to determine how well students did in the new XP course verses the plan- driven course [6]. Through questionnaires data was gathered that measured team cohesion and attachment to the project. While no statistical analysis can be done on the data, there are potential effects of the differing methodologies indicated by the data. First, the plan- driven group members were close knit within the sub- components of the project while the XP group was close knit across the whole group. Second, the XP group was more excited at the start of the project, but by the end of the project had the same excitement level as the plan-driven group. Lastly, while the XP code was structurally better, the plan-driven group had a much better user interface [6].

3. Agile topics in our metrics/management course
Our software engineering concentration includes a course titled Software Metrics/Process Management whose course description is:
This course explores and evaluates current methods of measuring the software development process. The need for such measurement is motivated by application of the Personal Software Process. Techniques for gathering and analyzing common metrics and their use in support process improvement. Clearly, this course includes some plan-driven topics as it includes the Personal Software Process. As part of complying with ABET requirements, one fourth of this course is mathematical content, so the textbook for the course is a book on statistical process control and the statistics is a critical part of the course. Surprisingly, this course has become key to helping our students understand the implications of process decisions.
This course has only lower division pre-requisites, so we cannot assume that the students have had either of the product development courses. This means that we have to start the class by talking about what we mean by “process.” After outlining CMM to build some of the necessary jargon, the students read and analyze Royce’s original waterfall paper [4]. The students have seen the waterfall model (what CS 1 text does not include it). They are quite surprised to find its weaknesses and the fact that those weaknesses were recognized so long ago. This is the beginning of encouraging them to question the process.
The next paper the students read is Reeves’ paper on “What is Software Design?” [3] On their first reading, they think Reeves is absolutely correct and the rest of us are idiots for not seeing it. We then ask them to find the holes in Reeves’ argument or his presentation of that argument. The students noted that viewing software design as a process verses a concrete document had a big effect on Reeves’ argument and that he left this point unclear in the paper. The students then begin to see that the debate isn’t so black and white.
The point of reading and discussing these (and other similar papers) is to build the students’ ability to read software engineering literature critically and to analyze when a particular technique may or may not be beneficial.
After we have clarified the concept of process, we pursue statistical process control with rigor. The techniques associated with statistical process control are based on the assumption that, if the process is under control, then the metrics will be consistent over time and changes in those values signify a significant event in the process. In plan-driven methods, since the activities the team is involved with change over time, the metrics also change over time. For example, during requirements analysis, the amount of code being produced is, and is expected to be quite small, but that will change later in the process. In agile methods, the same activities happen in every iteration, so the metrics do exhibit the consistency expected by statistical process control.
In doing examples and exercises, the students learn the ability to detect trends and anomalies in metrics gathered over time. The students recognize that only an agile process would exhibit the consistency that is necessary for these techniques to work. They find it highly interesting that statistical process control, which originated in the plan-driven community, applies so much better to agile processes.



4. Agile in CS I and CS II
Our XP course has generated a good amount of discussion among our students, our faculty members, and our alumni. These conversations have led to a number of interesting innovations to the way we teach some of our courses.
4.1. Pair Programming
Many of our faculty have had students program in pairs in the introductory level courses. In fact, until recently, our computer classrooms had half as many computers as seats. However, this pairing was done with little planning or training of the students. This meant that we encountered all of the weaknesses this can bring: pairs who are unevenly participating, students who are uninvolved, and pairs that become too dependent on each other.
Our initial motivation for these pairings was pedagogical. At the time, we were part of the Mathematics department and our colleagues were very interested in using small group problem solving activities to increase the engagement of the students and their analytical skills. We used programming together for similar reasons.
The XP literature is very specific about some key aspects of pair programming that we were not employing [1]: changing roles of pilot and co-pilot and changing partners regularly. By realizing that we were really aiming for pair programming – not just small group activities – we recognized the need for these changes in the dynamics of the student pairs.
By having students switch partners between projects it forced the spread of knowledge. By forcing students to take time in the pilot char it makes the better students become co-pilots and therefore explain to the weaker students what needs to be done. We believe this has improved the performance of students in these courses.
4.2. Automated testing in CS II
Some of our faculty members have become enamored with automated testing. One faculty member built an on-line system that tested students’ projects. The students could submit their programs as often as they liked and could see which tests passed and which tests failed. The system provided rapid feedback to the students about the functionality they had built. Unfortunately, the system did not improve student performance as initially expected [2]. As will be explained below, when the process was analyzed more closely a number of problems were discovered with it.
There were some key differences between this course and the test-driven development (TDD) approach, which based on student assessment, contributed to a negative outcome. In this approach, students were assigned a series of labs. Each lab defined a problem, and a set of criteria. For example, one program involved making change using the optimum number of coins. The specifications were rigid, in that students had to implement a set of methods such as int getDimes() [2].
The tests were developed by the course instructor. There were two types of tests, value based and introspection based. The first set of tests are intuitive. For the coin changing example, given a value, did they compute and return the correct number of coins. The second type of test used Java's introspection capabilities to allow runtime binding and inspection of the student’s code. These tests ensured that students provided the specified program methods, and that there were no unnecessary public methods exposed [2].
In this scenario, students submitted their source code through a web page. Their code was compiled and the set of tests were applied. Students would receive a status indicating whether their code passed or failed, and indicated the result of each test. A diagnostic message such as test failed: getDimes() was incorrect would be displayed [2].
This blind approach did not allow students to develop their own test cases. Additionally, when their program failed they only had limited information as to the cause(s). So while the students had the tests first, they didn't understand how the tests were created or why their programs failed and therefore didn't really understand what changes to make. The result was students making changes without any reasoning behind them, rendering the tests useless and in some cases detrimental. In the most extreme case, one student submitted nearly identical code over one-hundred times, each time, modifying the return value of getDimes() by one, in hopes of passing the tests. By allowing students a true test-first approach these mistakes can be avoided [2].
4.3. Test-Driven Development in CS I
From prep work done to teach the new XP course, one of our faculty members has begun experimenting with TDD in CS I. He taught two sections: one using the traditional method and one incorporating TDD using JUnit. While this experiment was preliminary, we see great potential for expanding on this early success.
In particular, TDD adds to CS I in two ways. Early in the semester, it gives us the ability to have the students practice using variables and control structures with the repetition that math courses use. We can create many JUnit tests that require the students to develop methods containing various coding techniques. Since the students are not developing entire applications, they focus on the particular coding technique and can complete a large number of exercises. For example, they can complete a lab that requires them to write 20 different if-then-else structures.
Later in the semester, the students start writing the JUnit tests themselves. This is their first attempt at “design” in that it is the first time they have thought about what they should code on their own before they code it. While they learn some basic UML diagrams, they are really designing at a lower level than those diagrams support (it is rare for a project to have more than a few classes). At this stage in their development, the detailed design represented by JUnit tests serves them well.
To help the students more easily adopt a test-driven approach in CS I two faculty have designed and built two wizards for Eclipse [2]. The first wizard supplants the existing new class wizard that comes with Eclipse. This wizard asks students to layout the class, including any variables and any methods. The wizard generates an empty framework class, including the defined variables, each given a default variable, an accessor and mutator for each of these variables, and then any additional methods defined by the user. The code is filled in with defaults so as to compile and be suitable for use by the test cases [2].
The TDL Wizard guides students through creating a test case. The wizard generates the necessary classes, modifies the project's build path to include JUnit, and automatically imports the JUnit packages into the test class. Each of these tasks would likely be confusing to the new CS I student. The TDL Wizard then helps students implement tests by generating the code for them. Using this wizard, students are free to generate tests for methods and instance variables without worrying about generating code. This is important for those students who are having difficulty, as it enables them to think about the behavior, and not worry about the implementation [2].


5.The Future of Agility in our Program
While the creep of agility into other courses has been positive, our students still do not have the skills necessary to really succeed in the XP course or in an agile industry team. In particular, they are missing many of the skills required by incremental design. Our curriculum contained some information on design patterns and refactoring, but only as ancillary topics. We are working to remedy this situation by turning our third course into a combination design patterns and refactoring course. This course will be introduced in the fall with the hopes that these skills will improve our students’ abilities to design and build quality systems incrementally.
5.1. What is missing?
Agile development certainly requires skills that were not required by plan-driven development. Certainly, one course in design patterns and refactoring is unlikely to give them the experience necessary to build truly well designed systems incrementally. Similarly, the early experiences with automated testing (which are not continued throughout our program) will not make them experts in TDD. While they have developed mock objects, there are many subtleties of TDD they do not begin to understand.
Our program does not currently contain any automated acceptance test content except in selected topics courses. This is clearly a capability an agile developer should master.
Finally, while our students use CVS and CruiseControl in the XP course, we don’t explicitly cover continuous integration and the tools that support it. In fact, since all of the tools are selected and in place at the beginning of each of our course, our students do not learn how to compare tools, to select a tool, and to install and manage those tools. These are also skills used by good agile developers.


6. Conclusion
While agility has added energy and innovation to much of our curriculum, there is a fine line between taking it too far. Much of the industry still relies on plan driven methods, so our students need to be prepared to succeed in those environments. Therefore, we cannot eliminate plan-driven experiences from the curriculum. However, there are a number of other motivations for continuing to teach plan-driven methods.
Our interest in continuing to teach plan-driven methods is rooted in the philosophy that software engineers are responsible for two things: improving the process and producing high quality software.
We believe that our software engineering students will continue to benefit from seeing widely different processes for a long time. Having experiences in both plan-driven and agile projects gives the students a broader understanding of what a development process can be and prepares them for questioning and improving the development process throughout their careers. Only after they see their second development process do our students begin to see the wide-ranging side effects of the process. That experience is critical to their ability to analyze the quality of a process and make changes to improve the process.
There is also an argument for teaching plan-driven requirements analysis and design even if it weren’t used in industry. We continue to teach computer science students assembly language programming because it helps them understand how the underlying machine works – not because we expect them to use in much in their careers. Similarly, we believe that good experience in UML for requirements analysis and design is critical to helping our students develop the reasoning skills necessary for good requirements analysis and design whether it is done up front or incrementally. It also helps develop the language of designs and the ability to distinguish good and bad designs.
While we started by injecting XP into our software engineering concentration, agile methods have begun to creep into a wide variety of our courses. We have
seen an increase in agile concepts in our existing courses and have created new courses designed to better prepare our students for agile environments. We are now at a point where we consider introducing more agile content carefully as we do not think our students would be well served by a program that was exclusively agile.
